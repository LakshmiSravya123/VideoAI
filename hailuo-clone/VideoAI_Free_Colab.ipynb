{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üé¨ VideoAI - Free Video Generation\n",
        "\n",
        "## 100% Free AI Video Generation using Google Colab!\n",
        "\n",
        "**Features:**\n",
        "- ‚úÖ Completely Free\n",
        "- ‚úÖ No Credit Card Required\n",
        "- ‚úÖ High Quality Videos\n",
        "- ‚úÖ Multiple Models\n",
        "\n",
        "**Instructions:**\n",
        "1. Click Runtime ‚Üí Run all\n",
        "2. Wait 2-3 minutes for setup\n",
        "3. Enter your prompt in the last cell\n",
        "4. Generate and download your video!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "install"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Installing dependencies...\n",
            "zsh:1: no matches found: imageio[ffmpeg]\n",
            "‚úÖ Dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "# Step 1: Install Dependencies\n",
        "print(\"üì¶ Installing dependencies...\")\n",
        "!pip install -q diffusers transformers accelerate imageio[ffmpeg] flask flask-cors pyngrok\n",
        "print(\"‚úÖ Dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "imports"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/sravyalu/VideoAI/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported!\n",
            "üî• GPU Available: False\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Import Libraries\n",
        "import torch\n",
        "from diffusers import CogVideoXPipeline\n",
        "from diffusers.utils import export_to_video\n",
        "import imageio\n",
        "from IPython.display import Video, display\n",
        "import os\n",
        "\n",
        "print(\"‚úÖ Libraries imported!\")\n",
        "print(f\"üî• GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"   GPU Name: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "load_model"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü§ñ Loading CogVideoX-2B model...\n",
            "‚è≥ This may take 2-3 minutes...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Fetching 14 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [02:32<00:00, 10.88s/it]\n",
            "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:03<00:00,  1.89s/it]\n",
            "Loading pipeline components...:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [00:06<00:01,  1.53s/it]\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "The component <class 'transformers.models.t5.tokenization_t5._LazyModule.__getattr__.<locals>.Placeholder'> of <class 'diffusers.pipelines.cogvideo.pipeline_cogvideox.CogVideoXPipeline'> cannot be loaded as it does not seem to have any of the loading methods defined in {'ModelMixin': ['save_pretrained', 'from_pretrained'], 'SchedulerMixin': ['save_pretrained', 'from_pretrained'], 'DiffusionPipeline': ['save_pretrained', 'from_pretrained'], 'OnnxRuntimeModel': ['save_pretrained', 'from_pretrained'], 'PreTrainedTokenizer': ['save_pretrained', 'from_pretrained'], 'PreTrainedTokenizerFast': ['save_pretrained', 'from_pretrained'], 'PreTrainedModel': ['save_pretrained', 'from_pretrained'], 'FeatureExtractionMixin': ['save_pretrained', 'from_pretrained'], 'ProcessorMixin': ['save_pretrained', 'from_pretrained'], 'ImageProcessingMixin': ['save_pretrained', 'from_pretrained'], 'ORTModule': ['save_pretrained', 'from_pretrained']}.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mü§ñ Loading CogVideoX-2B model...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚è≥ This may take 2-3 minutes...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m pipe = \u001b[43mCogVideoXPipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTHUDM/CogVideoX-2b\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available():\n\u001b[32m     11\u001b[39m     pipe.to(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/VideoAI/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/VideoAI/.venv/lib/python3.13/site-packages/diffusers/pipelines/pipeline_utils.py:1025\u001b[39m, in \u001b[36mDiffusionPipeline.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m   1018\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1019\u001b[39m     \u001b[38;5;66;03m# load sub model\u001b[39;00m\n\u001b[32m   1020\u001b[39m     sub_model_dtype = (\n\u001b[32m   1021\u001b[39m         torch_dtype.get(name, torch_dtype.get(\u001b[33m\"\u001b[39m\u001b[33mdefault\u001b[39m\u001b[33m\"\u001b[39m, torch.float32))\n\u001b[32m   1022\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(torch_dtype, \u001b[38;5;28mdict\u001b[39m)\n\u001b[32m   1023\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m torch_dtype\n\u001b[32m   1024\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1025\u001b[39m     loaded_sub_model = \u001b[43mload_sub_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclass_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimportable_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimportable_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpipelines\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipelines\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mis_pipeline_module\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_pipeline_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpipeline_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpipeline_class\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43msub_model_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43msess_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43msess_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurrent_device_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1036\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1037\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1038\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_state_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1039\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_variants\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_variants\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1040\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1041\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1042\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1043\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlow_cpu_mem_usage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1044\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcached_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1045\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdduf_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdduf_entries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1050\u001b[39m     logger.info(\n\u001b[32m   1051\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m as \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m from `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` subfolder of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1052\u001b[39m     )\n\u001b[32m   1054\u001b[39m init_kwargs[name] = loaded_sub_model  \u001b[38;5;66;03m# UNet(...), # DiffusionSchedule(...)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/VideoAI/.venv/lib/python3.13/site-packages/diffusers/pipelines/pipeline_loading_utils.py:770\u001b[39m, in \u001b[36mload_sub_model\u001b[39m\u001b[34m(library_name, class_name, importable_classes, pipelines, is_pipeline_module, pipeline_class, torch_dtype, provider, sess_options, device_map, max_memory, offload_folder, offload_state_dict, model_variants, name, from_flax, variant, low_cpu_mem_usage, cached_folder, use_safetensors, dduf_entries, provider_options, quantization_config)\u001b[39m\n\u001b[32m    766\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_dummy_path \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdummy\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m none_module:\n\u001b[32m    767\u001b[39m         \u001b[38;5;66;03m# call class_obj for nice error message of missing requirements\u001b[39;00m\n\u001b[32m    768\u001b[39m         class_obj()\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    771\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe component \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_obj\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpipeline_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cannot be loaded as it does not seem to have\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    772\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m any of the loading methods defined in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mALL_IMPORTABLE_CLASSES\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    773\u001b[39m     )\n\u001b[32m    775\u001b[39m load_method = _get_load_method(class_obj, load_method_name, is_dduf=dduf_entries \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    777\u001b[39m \u001b[38;5;66;03m# add kwargs to loading method\u001b[39;00m\n",
            "\u001b[31mValueError\u001b[39m: The component <class 'transformers.models.t5.tokenization_t5._LazyModule.__getattr__.<locals>.Placeholder'> of <class 'diffusers.pipelines.cogvideo.pipeline_cogvideox.CogVideoXPipeline'> cannot be loaded as it does not seem to have any of the loading methods defined in {'ModelMixin': ['save_pretrained', 'from_pretrained'], 'SchedulerMixin': ['save_pretrained', 'from_pretrained'], 'DiffusionPipeline': ['save_pretrained', 'from_pretrained'], 'OnnxRuntimeModel': ['save_pretrained', 'from_pretrained'], 'PreTrainedTokenizer': ['save_pretrained', 'from_pretrained'], 'PreTrainedTokenizerFast': ['save_pretrained', 'from_pretrained'], 'PreTrainedModel': ['save_pretrained', 'from_pretrained'], 'FeatureExtractionMixin': ['save_pretrained', 'from_pretrained'], 'ProcessorMixin': ['save_pretrained', 'from_pretrained'], 'ImageProcessingMixin': ['save_pretrained', 'from_pretrained'], 'ORTModule': ['save_pretrained', 'from_pretrained']}."
          ]
        }
      ],
      "source": [
        "# Step 3: Load Model (CogVideoX-2B - Smaller, faster)\n",
        "print(\"ü§ñ Loading CogVideoX-2B model...\")\n",
        "print(\"‚è≥ This may take 2-3 minutes...\")\n",
        "\n",
        "pipe = CogVideoXPipeline.from_pretrained(\n",
        "    \"THUDM/CogVideoX-2b\",\n",
        "    torch_dtype=torch.float16\n",
        ")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    pipe.to(\"cuda\")\n",
        "    print(\"‚úÖ Model loaded on GPU!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Running on CPU (will be slower)\")\n",
        "\n",
        "print(\"üé¨ Ready to generate videos!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate_function"
      },
      "outputs": [],
      "source": [
        "# Step 4: Generate Video Function\n",
        "def generate_video(prompt, output_path=\"output.mp4\", num_frames=49):\n",
        "    \"\"\"\n",
        "    Generate a video from a text prompt\n",
        "    \n",
        "    Args:\n",
        "        prompt: Text description of the video\n",
        "        output_path: Where to save the video\n",
        "        num_frames: Number of frames (default 49 = ~6 seconds)\n",
        "    \"\"\"\n",
        "    print(f\"üé® Generating video for: {prompt}\")\n",
        "    print(f\"‚è≥ This will take 30-60 seconds...\")\n",
        "    \n",
        "    # Generate\n",
        "    video_frames = pipe(\n",
        "        prompt=prompt,\n",
        "        num_frames=num_frames,\n",
        "        guidance_scale=6.0,\n",
        "        num_inference_steps=50\n",
        "    ).frames[0]\n",
        "    \n",
        "    # Save\n",
        "    export_to_video(video_frames, output_path, fps=8)\n",
        "    \n",
        "    print(f\"‚úÖ Video saved to: {output_path}\")\n",
        "    \n",
        "    # Display\n",
        "    display(Video(output_path, embed=True, width=512))\n",
        "    \n",
        "    return output_path\n",
        "\n",
        "print(\"‚úÖ Generation function ready!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "generate_header"
      },
      "source": [
        "---\n",
        "## üé¨ Generate Your Video!\n",
        "\n",
        "**Edit the prompt below and run the cell:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate"
      },
      "outputs": [],
      "source": [
        "# üé¨ EDIT THIS PROMPT:\n",
        "prompt = \"A golden retriever running through a field of sunflowers at sunset\"\n",
        "\n",
        "# Generate!\n",
        "video_path = generate_video(prompt)\n",
        "\n",
        "# Download link\n",
        "from google.colab import files\n",
        "print(\"\\nüì• Click below to download your video:\")\n",
        "files.download(video_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "examples"
      },
      "source": [
        "---\n",
        "## üí° Example Prompts\n",
        "\n",
        "Try these prompts:\n",
        "\n",
        "```python\n",
        "# Nature\n",
        "\"A majestic waterfall cascading down rocks in a lush rainforest\"\n",
        "\n",
        "# Animals\n",
        "\"A cat playing with a ball of yarn on a cozy carpet\"\n",
        "\n",
        "# Urban\n",
        "\"City street at night with cars and neon lights\"\n",
        "\n",
        "# Fantasy\n",
        "\"A dragon flying over a medieval castle at dawn\"\n",
        "\n",
        "# Action\n",
        "\"A sports car drifting around a corner on a race track\"\n",
        "```\n",
        "\n",
        "**Just copy a prompt above and paste it in the previous cell!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "api_header"
      },
      "source": [
        "---\n",
        "## üöÄ Optional: Create API Server (Advanced)\n",
        "\n",
        "Run this to create a temporary API endpoint you can connect to from your app!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "api_server"
      },
      "outputs": [],
      "source": [
        "# Optional: Create Flask API + ngrok tunnel\n",
        "from flask import Flask, request, jsonify, send_file\n",
        "from flask_cors import CORS\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "@app.route('/generate-video', methods=['POST'])\n",
        "def api_generate():\n",
        "    data = request.json\n",
        "    prompt = data.get('prompt', '')\n",
        "    \n",
        "    if not prompt:\n",
        "        return jsonify({'error': 'Prompt required'}), 400\n",
        "    \n",
        "    output_path = generate_video(prompt)\n",
        "    \n",
        "    return jsonify({\n",
        "        'video_url': f'/download/{output_path}',\n",
        "        'prompt': prompt\n",
        "    })\n",
        "\n",
        "@app.route('/download/<path:filename>')\n",
        "def download(filename):\n",
        "    return send_file(filename, mimetype='video/mp4')\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"\\nüåê Your API is live at: {public_url}\")\n",
        "print(f\"\\nüìù Use this URL in your frontend!\")\n",
        "print(f\"   Replace http://localhost:5000 with: {public_url}\")\n",
        "\n",
        "# Run Flask\n",
        "threading.Thread(target=lambda: app.run(port=5000, debug=False)).start()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
